# Lung Nodule Segmentation

This repository contains scripts to train and validate lung nodule segmentation models (U-Net, U-Net++) on the LIDC‑IDRI dataset, using preprocessed data generated by Jaeho’s preprocessing pipeline.

---

## Prerequisites

- **Dataset**: LIDC‑IDRI (CT scans)  
- **Python**: 3.6+
- **PyTorch**: 1.4 with CUDA support (GPU recommended)
- **Shell utilities**: `git`, `ssh`, `sbatch`

---

## 1. Download the LIDC‑IDRI Dataset

1. Visit the [LIDC‑IDRI collection](https://www.cancerimagingarchive.net/collection/lidc-idri/) on The Cancer Imaging Archive (TCIA).
2. In the **Data Access** section, click **Search**, filter by **Modality: CT**, and download all patient studies (1010 patients in total).
3. Unzip the downloaded archive so that you have a top‑level folder named `LIDC-IDRI/` containing all DICOM files.

---

## 2. Preprocess with Jaeho’s Pipeline

Jaeho’s preprocessing repository uses the `pylidc` library to extract nodule-containing slices and save them as NumPy (`.npy`) files.

1. **Clone the preprocessing repo** (replace `<preprocessing-repo-url>` with the actual URL):
   ```bash
   git clone <preprocessing-repo-url> lidc-preprocessing
   cd lidc-preprocessing
   ```

2. **Install dependencies**:
   ```bash
   pip install pylidc
   ```

3. **Configure**:
   ```bash
   python config_file_create.py
   ```
   This generates a `lung.conf` file. Edit it to set your input (`LIDC-IDRI/`) and output (`data/`) directories.

4. **Run preprocessing**:
   ```bash
   python prepare_dataset.py
   ```
   - Generates `.npy` images and masks under `data/`.
   - Creates `meta.csv` with nodule malignancy labels and split flags.

5. **Create train/val/test splits**:
   ```bash
   jupyter notebook notebook/split_data.ipynb
   ```
   This notebook outputs `clean_meta.csv` and updated `meta.csv` with splits that keep all slices of the same nodule together.

---

## 3. Segmentation (This Repository)

1. **Clone this repository**:
   ```bash
   git clone https://github.com/<yourusername>/Segmentation.git
   cd Segmentation
   ```

2. **Set up the virtual environment**:
   ```bash
   python -m venv myenv
   source myenv/bin/activate
   pip install -r requirements.txt
   ```

3. **Submit jobs on the university batch system**:
   ```bash
   ssh kudu                              # log into the compute cluster
   cd /path/to/Segmentation              # navigate to this repo
   source myenv/bin/activate             # activate your environment

   # Training (U-Net or U-Net++)
   sbatch train.sbatch --name UNET --augmentation True

   # Validation
   sbatch validate.sbatch --name UNET --augmentation True
   ```

> **Note:** Both `train.sbatch` and `validate.sbatch` accept additional arguments:
> - `--name` (model name: `UNET` or `NestedUNET`)
> - `--augmentation` (`True` or `False`)
> - `--epochs`, `--batch-size`, `--lr`, etc.

---

## 4. Repository Structure

```
Segmentation/
├── train.sbatch            # SLURM script for training
├── validate.sbatch         # SLURM script for validation
├── train.py                # Training entrypoint
├── validate.py             # Validation entrypoint
├── Unet/                   # U-Net model code
├── UnetNested/             # U-Net++ model code
├── dataset.py              # PyTorch Dataset for .npy files
├── losses.py               # BCE + Dice loss
├── metrics.py              # Dice, IOU metrics
├── utils.py                # Utility functions
├── requirements.txt        # Python dependencies
└── figures/                # Saved training/validation figures
```

---

## 5. Acknowledgements

- **Preprocessing pipeline** by Jaeho (uses `pylidc`).
- **Reference implementation** by Mike Huang: https://github.com/mikejhuang/LungNoduleDetectionClassification

---

## 6. Contributing

Feel free to open issues or submit pull requests. If you find this repository useful, please ⭐ star it!

---

## License

This project is licensed under the MIT License — see the [LICENSE](LICENSE) file for details.

